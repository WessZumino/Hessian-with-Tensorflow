{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Exact Hessian of Deep Neural Networks: Autograd v.s. Tensorflow\n",
        "\n",
        "This is a tutorial style evaluation of the Hessian matrix for multi layer Neural Networks. We do this both in autograd and in Tensorflow. Evaluation using the former has been partially ducumented elsewhere, and seems more established, while I did not find a correct implementation of the latter. Indeed, you will find that the `tf.hessian()` function is not evaluating the exact hessian.   \n",
        "\n",
        "In order to simplify things and being able to directly inspect the results, we consider a simple 2 layer linear network with Nfeat =3 input features, Nh = 2 hidden units and Nlab=1 output. Let us start with Autograd   \n",
        "\n![alt text](FF1.pdf)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear network is defined by two simple linear operations. The linear output of the two layers is denotes as \n",
        "\n",
        "$$h^{[1]}_i = \\sum_j w^{[1]}_{ij} \\, X_j + b^{[1]}_i $$ \n",
        "\n",
        "and \n",
        "\n",
        "$$h^{[2]}_i = \\sum_j w^{[2]}_{ij} \\, h^{[1]}_j + b^{[2]}_i, $$\n",
        "\n",
        "where the superscipt denotes the layer and the lower indices denote the elements of the vector/matrix. Finally, we need to define the Loss function. Here we use the minimum squared error loss, but you can use your favourite one. \n",
        "\n",
        "$$ L = \\frac{1}{2 m } \\sum_{\\mu} (\\hat{y}^{\\mu} - y^{\\mu} )^2 $$ \n",
        "where in this simple case $\\hat{y} = h_2$\n",
        "\nNote that in this tutorial we are not going to implement any optimization routine, but merely show how to evaluate the Hessian for a fixed value of the parameters. You can easily implement the code here to work with your gradient descent algo."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hessian** In general, defining the vector $\\theta_{\\alpha} = (\\mathbf{w}_1, \\mathbf{b}_1, \\mathbf{w}_2, \\mathbf{b}_2)$, the Hessian matrix is given by \n",
        "\n\n",
        "$$\n",
        "H_{\\alpha, \\beta} = \\frac{\\partial^2 L}{\\partial \\theta_{\\alpha} \\, \\partial \\theta_{\\beta}} =\n",
        "\\begin{pmatrix}\n",
        "\\frac{\\partial^2 L}{\\partial w^{[1]}_{ij} \\, \\partial w^{[1]}_{lm}} & \\frac{\\partial^2 L}{\\partial w^{[1]}_{ij} \\, \\partial b^{[1]}_{lm}} & \\frac{\\partial^2 L}{\\partial w^{[1]}_{ij} \\, \\partial w^{[2]}_{lm}} & \\frac{\\partial^2 L}{\\partial w^{[1]}_{ij} \\, \\partial b^{[2]}_{l}}\\\\\n",
        "\\frac{\\partial^2 L}{\\partial b^{[1]}_{i} \\, \\partial w^{[1]}_{lm}} &  \\frac{\\partial^2 L}{\\partial b^{[1]}_{i} \\, \\partial b^{[1]}_{j}} &  \\frac{\\partial^2 L}{\\partial b^{[1]}_{i} \\, \\partial w^{[2]}_{lm}} &  \\frac{\\partial^2 L}{\\partial b^{[1]}_{i} \\, \\partial b^{[2]}_{j}}  \\\\\n",
        "\\frac{\\partial^2 L}{\\partial w^{[2]}_{ij} \\, \\partial w^{[1]}_{lm}} & \\frac{\\partial^2 L}{\\partial w^{[2]}_{ij} \\, \\partial b^{[1]}_{l}} & \\frac{\\partial^2 L}{\\partial w^{[2]}_{ij} \\, \\partial w^{[2]}_{lm}} &  \\frac{\\partial^2 L}{\\partial w^{[2]}_{ij} \\, \\partial b^{[2]}_{l}} \\\\ \n",
        "\\frac{\\partial^2 L}{\\partial b^{[2]}_{i} \\, \\partial w^{[1]}_{lm}} & \\frac{\\partial^2 L}{\\partial b^{[2]}_{i} \\, \\partial b^{[1]}_{j}} & \\frac{\\partial^2 L}{\\partial b^{[2]}_{i} \\, \\partial w^{[2]}_{lm}} &  \\frac{\\partial^2 L}{\\partial b^{[2]}_{i} \\, \\partial b^{[2]}_{j}}\n",
        "\\end{pmatrix} = \n",
        "\\begin{pmatrix} \n",
        "H_{11} & H_{12} \\\\\n",
        "H_{21} & H_{22}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "where in the last identity we have used a compact, block notation in layer space. Note that each element appearing in the big matrix is itself a matrix. The off-diagonal blocks are important if oyu want to capture correlations between different layers, that are crucial to the deep learning framework. With respect to this, I have seen in the literature something called \"diagonal approximation\", however to me it looks like a rather bold step. Clarly, if the off-diagonal elements are small, you can obtain an approximate block diagonal form in which the Hessian factorizes. Reaining only the block diagonal part means neglecting correlations between different layers, i.e. saying that those are not important! But these are at the core of the Deep Learning framework ... a perturbative expansion it is certainly more appropriate.  \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we want to compare results with the tensor flow implementation, we need to make sure that the weights are initialized in the same way in both implementations. Below we first define the input matrix and initialize the weights and biases by sampling from a normal distirbution. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np #autograd version of numpy\n",
        "\n",
        "x = np.array([[0.52, 1.12,  0.77],\n",
        "             [0.88, -1.08, 0.15],\n",
        "             [0.52, 0.06, -1.30],\n",
        "             [0.74, -2.49, 1.39]])\n",
        "\n",
        "m, Nfeat = x.shape # m = number of training sampples, f = number of features\n",
        "\n",
        "#Define the ouput\n",
        "y= x[:,0]**2-3*x[:,1]*x[:,2]\n",
        "y = np.reshape(y, (m,1))\n",
        "\n",
        "_, Nlab = y.shape #number of units in the ouput layer\n",
        "\n",
        "Nh= 2  #number of units in the hidden layer\n",
        "\n",
        "#initialize parameters to random value\n",
        "\n",
        "W1 = np.random.randn(2,3)\n",
        "B1 = np.random.randn(2,1)\n",
        "\n",
        "W2 = np.random.randn(1,2)\n",
        "B2 = np.random.randn(1,1)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd\n",
        "\n[Autograd](https://github.com/WessZumino/autograd) is a nice library that allows you to perform automatic differentiation on native Python and Numpy code. We first import the relevant libraries  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from flatten import*\n",
        "from autograd import grad, hessian"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define first some useful helper functions to evaluate the linear ouput of the network and the loss function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear ouputs\n",
        "def nn_outs(params, inputs):\n",
        "    for W, b in params:\n",
        "        outputs = np.dot(W, inputs) + b\n",
        "        inputs = outputs\n",
        "    return outputs\n",
        "\n",
        "#Loss function for the regression problem\n",
        "def loss(weights, input, target):\n",
        "\n",
        "        h2 = nn_outs(weights, input)\n",
        "\n        return np.mean(np.square(h2-target))/2"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation\n",
        "\n",
        "This is quite straightforward in Autograd, where we will use the native Hessian function\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize paramters: returns 4 params\n",
        "init_params = [(W1, B1), (W2, B2)]\n",
        "\n",
        "#Initial value of the objective funciton\n",
        "loss(init_params, x.T, y.T)\n",
        "\n",
        "#evaluate the gradients\n",
        "grads_ag = grad(objective)(init_params, x.T, y.T)\n",
        "\n",
        "#returns a list with four arrays containing dw1, db1, dw2, db2\n",
        "print('Gradients:',grads_ag)\n",
        "\n",
        "#flatten the objective + initialization paramters. This step is needed to pass the input as a one dimensional \n",
        "#array to the evaluate Hessian function.\n",
        "\n",
        "flat_f, unflatten, flat_params = flatten_func(objective, init_params)\n",
        "\n",
        "print('flattened parameters:',flat_params)\n",
        "\n",
        "#evaluate the Hessian\n",
        "flat_hess = hessian(flat_f) #initialize the Hessian\n",
        "\n",
        "hess_ag = flat_hess(flat_params, x.T, y.T) #evaluates the Hessian on the input/ouput data\n",
        "\n",
        "#we can now check that this has the expected dimensions: (2*3)+(2*1)+(1*2)+1 = 11 --> hess= (11,11)\n",
        "print('dimension of the Hessian matrix:',hess_ag[0].shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients: [(array([[-0.20980657, -5.42853043,  1.58130118],\n",
            "       [ 0.20253104,  5.24028375, -1.52646595]]), array([[-0.75951313],\n",
            "       [ 0.73317528]])), (array([[-4.41988815, -3.68119392]]), array([[0.76095164]]))]\n",
            "flattened parameters: [-0.38575379 -0.62680877  0.19602057  0.34699285 -0.51387488  1.07746459\n",
            " -0.81365418  0.98266259 -0.99810959  0.96349786  0.98364881]\n",
            "dimension of the Hessian matrix: (11, 11)\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to implement the same calculation using tensorflow (TF). I assume you have a basic knowlede if it, but I will try to be comprehensive anyway"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Applications/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Network\n",
        "\n",
        "Unlike in the previous example, in TF one works with symbolic tensors, values are passed to them at running time. This is like when you solve a physics/math problem with pen and paper! Also in this case it is convenient to carry on calculations with symbolic variables until the very end, where you are going to input the actual values of these variables.\n",
        "\nThese symbolic tensors are called place holders in TF for obvious reasons. We define one for the input and one for the output. Note that \"None\" means that we do not have to specify the number of training examples in the dataset for the moment (in our case this is trivial, m=1).   "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(shape= [Nfeat, None], dtype= \"float64\", name= 'X' )\n",
        "Y = tf.placeholder(shape= [Nlab, None],  dtype= \"float64\", name= 'Y' )"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the variables**. For each layer we need to specify the weights (i.e the connections) and the biases. So for this simple case we have a total of 4 parameters. As we want to compare with the Autograd result, we initialize the weights in the same way "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Define first the initializers: \n",
        "\n",
        "w1_init = tf.constant_initializer(W1)\n",
        "b1_init = tf.constant_initializer(B1)\n",
        "\n",
        "w2_init= tf.constant_initializer(W2)\n",
        "b2_init= tf.constant_initializer(B2)\n",
        "\n",
        "#Use them to initialized the weights and bias:\n",
        "w1= tf.get_variable('w1', [Nh, Nfeat], dtype= 'float64' ,initializer = w1_init )\n",
        "b1= tf.get_variable('b1', [Nh, 1], dtype= 'float64', initializer = b1_init)\n",
        "\n",
        "w2= tf.get_variable('w2', [Nlab, Nh], dtype= 'float64' ,initializer = w2_init )\n",
        "b2= tf.get_variable('b2', [Nlab, 1], dtype= 'float64', initializer = b2_init )\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward pass and Loss**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the FW pass to the two units\n",
        "h1 = tf.matmul(w1, X)+b1\n",
        "h2 = tf.matmul(w2, h1)+b2\n",
        "\n",
        "#define the loss\n",
        "L = tf.reduce_mean(tf.squared_difference(h2, Y))/2\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on, we get a list of all the trainable variables "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tvars = tf.trainable_variables()\n",
        "print('Trainable parametrs:',tvars)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parametrs: [<tf.Variable 'w1:0' shape=(2, 3) dtype=float64_ref>, <tf.Variable 'b1:0' shape=(2, 1) dtype=float64_ref>, <tf.Variable 'w2:0' shape=(1, 2) dtype=float64_ref>, <tf.Variable 'b2:0' shape=(1, 1) dtype=float64_ref>]\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operations on the Network : Gradients and Hessian matrix \n",
        "\nHere we define all the operations we want to perform. The **gradient** of the Loss function with respect to the training variables is easily obtained using the `tf.gradients()` function. Note: do not use $[0]$ as shown in some other tutorials, otherwise you get only dw1!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dL = tf.gradients(L, tvars )\n",
        "print('Gradients:',dL)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients: [<tf.Tensor 'gradients/MatMul_grad/MatMul:0' shape=(2, 3) dtype=float64>, <tf.Tensor 'gradients/add_grad/Reshape_1:0' shape=(2, 1) dtype=float64>, <tf.Tensor 'gradients/MatMul_1_grad/MatMul:0' shape=(1, 2) dtype=float64>, <tf.Tensor 'gradients/add_1_grad/Reshape_1:0' shape=(1, 1) dtype=float64>]\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the ouput, dL is a list containing ($ \\partial_{w1} L,\\, \\partial_{b1} L,\\, \\partial_{w2} L,\\, \\partial_{b2} L $ ). So the first index $dL[i]$ selects one of the 4 gradients. The other indices give access to the elements of the gradients: e.g. $d_{w1}$ is a (2,3) matrix, so $dL[0][i][j]$ gives access to the $i,j$ elements of the matrix.   \n",
        "\n",
        "**Hessian**: This part is a bit more tedious as the `tf.gradients()` function does not differentiate tensors of rank>1. A simple way of avoiding this problem is to pass slices of the orginal gradients (computed before) such that we pass each element one by one. In order to do that we first create a flattenning fucntion, just like the one that was provided in the Autograd implementation:\n",
        "\n"
      ],
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flattening function\n",
        "\n",
        "def flatten(tensor):\n",
        "\n",
        "    '''\n",
        "    Flattening function:\n",
        "\n",
        "    input: a tensor list\n",
        "    returns: a rank one tensor\n",
        "    '''\n",
        "\n",
        "    s= len(tensor) #number of tensors in the list\n",
        "\n",
        "    for i in range(s):\n",
        "\n",
        "        dl = tensor[i] #take one element of the list\n",
        "        d1, d2 = dl.get_shape() #Obtain tensor dimensions\n",
        "\n",
        "        fl = tf.reshape(dl,[-1, d1*d2]) #reshape the tensor to a (1, d1*d2) tensor\n",
        "\n",
        "        #concatenate over all the elemets in the list\n",
        "        if i==0: flattened = fl # the first time\n",
        "        else: flattened = tf.concat([flattened, fl], axis=1)\n",
        "\n",
        "    return flattened\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, we define another function to evaluate the Hessian"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Hessian function\n",
        "\n",
        "def hessian(grads, par):\n",
        "\n",
        "    '''\n",
        "    Evaluates the Hessian matrix\n",
        "\n",
        "    Inputs:\n",
        "    grads --- the evaluated gradeints of the cost function\n",
        "\n",
        "    Returns:\n",
        "    hessian matrix: a (dim,dim) matrix of second derivatives, where 'dim' is the dimension of\n",
        "    the flattened gradient tensor.\n",
        "    '''\n",
        "\n",
        "    flat_grads = flatten(grads)[0] #flat gradients\n",
        "\n",
        "    dim = flat_grads.get_shape()[0] #get the dimensions of the flattened tensor\n",
        "\n",
        "    hess = [] #list\n",
        "\n",
        "    for i in range (dim):\n",
        "\n",
        "        dg_i = tf.gradients(flat_grads[i], par) #for each element of grads evaluate the gradients\n",
        "        dg_i_flat = flatten(dg_i) #flatten the resulting hessian onto a 1 d array\n",
        "        hess.append(dg_i_flat) #store row by row\n",
        "\n",
        "    return tf.reshape(hess,[dim, dim]) #returns the reshaped matrix\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now apply the flattening function to the parameters and the gradients and use the result as an input to the Hessian function "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "flat_grads = flatten(dL)[0] #flattened gradients \n",
        "flat_par= flatten(tvars)[0] #flattened parameters\n",
        "\n",
        "#Evaluate the symbolic Hessian \n",
        "hess = hessian(dL, tvars) #evaluates the hessian\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we open a TF session in order to evaluate the symbolic Hessian on the data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer() #initialize the computational graph\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(init)\n",
        "    grads_tf, grads_flat, flat_params_tf, hess_tf = sess.run([dL, flat_grads, flat_par, hess], feed_dict = {X: x.T, Y: y.T} )\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the results from Autograd and Tensorflow\n",
        "\nwe can now print the result of the different operations we have perfomed so far, both in Autograd and Tensorflow "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#This is just to make sure we are using the same parameters \n",
        "print('Autograd parameters',flat_params)\n",
        "print('Tensorflow paramters', flat_params_tf )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autograd parameters [-0.38575379 -0.62680877  0.19602057  0.34699285 -0.51387488  1.07746459\n",
            " -0.81365418  0.98266259 -0.99810959  0.96349786  0.98364881]\n",
            "Tensorflow paramters [-0.38575379 -0.62680877  0.19602057  0.34699285 -0.51387488  1.07746459\n",
            " -0.81365418  0.98266259 -0.99810959  0.96349786  0.98364881]\n"
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluated Hessian matrix\n",
        "\n",
        "print( 'Hessian from Autograd:', hess_ag[0][10])\n",
        "print('Hessian from Tensorflow:', hess_tf[10] )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hessian from Autograd: [-0.66374288  0.59637048 -0.25202267  0.64072608 -0.57568997  0.24328321\n",
            " -0.99810959  0.96349786 -0.64616702  1.79251289  1.        ]\n",
            "Hessian from Tensorflow: [-0.66374288  0.59637048 -0.25202267  0.64072608 -0.57568997  0.24328321\n",
            " -0.99810959  0.96349786 -0.64616702  1.79251289  1.        ]\n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
